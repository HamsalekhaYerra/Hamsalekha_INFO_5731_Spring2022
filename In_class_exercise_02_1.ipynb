{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamsalekhaYerra/Hamsalekha_INFO_5731_Spring2022/blob/main/In_class_exercise_02_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zqj6WPzWhAG"
      },
      "source": [
        "## The third In-class-exercise (02/08/2022, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY8V_s4dWhAI"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU-VPjIcWhAJ"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5l-Na9ZWhAK"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection is a critical task for financial market, investors, and regulatory authorities, where conventional methods employ rule-based models. With the use of python it becomes more promising to detect anomalous trading behaviors from data.\n",
        "\n",
        "Hence, our task is to detect anamolies from share price data.\n",
        "\n",
        "The data recquired is historical share price data of any company.\n",
        "\n",
        "The attributes we need are Date, Open share Price, High, Low, Closing share price and Volume.\n",
        "\n",
        "To scrape the share price data of any company, we can use the Yahoo finance website to get the historical share price data.\n",
        "\n",
        "By entering the ticker symbol, yahoo finance gives us the corresponding company's share price data, which we can import into python and export into a csv file and do our further analysis."
      ],
      "metadata": {
        "id": "6ImLnofIcvCY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPNWzO1WWhAM"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VAzSyCg1WhAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae931672-0c09-4ce4-bc23-a10234676961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.70)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.7.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "                  Open        High  ...  Dividends  Stock Splits\n",
            "Date                                ...                         \n",
            "2012-05-18   42.049999   45.000000  ...          0             0\n",
            "2012-05-21   36.529999   36.660000  ...          0             0\n",
            "2012-05-22   32.610001   33.590000  ...          0             0\n",
            "2012-05-23   31.370001   32.500000  ...          0             0\n",
            "2012-05-24   32.950001   33.209999  ...          0             0\n",
            "...                ...         ...  ...        ...           ...\n",
            "2022-02-07  237.699997  238.300003  ...          0             0\n",
            "2022-02-08  220.850006  225.770004  ...          0             0\n",
            "2022-02-09  224.199997  233.369995  ...          0             0\n",
            "2022-02-10  228.270004  235.000000  ...          0             0\n",
            "2022-02-11  228.460007  230.419998  ...          0             0\n",
            "\n",
            "[2451 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "#Pip install yfinance\n",
        "!pip install yfinance --upgrade --no-cache-dir\n",
        "import yfinance as yf\n",
        "ticker = yf.Ticker(\"FB\") #we can enter any ticker, here for reference I have taken FB's share price data\n",
        "print(ticker.history(period=\"max\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSn-ZZaKWhAN"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqUZwiiCWhAO",
        "outputId": "e80ff738-e461-4213-bfca-973dac09fde3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests) (2.0.11)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests --upgrade\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
        "url = 'https://scholar.google.com/scholar?q=information+retrieval&hl=en&as_sdt=0%2C5&as_ylo=2012&as_yhi=2022'\n",
        "response=requests.get(url,headers=headers)"
      ],
      "metadata": {
        "id": "iNGSjy8hdkYC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing beautifulsoup\n",
        "!pip install beautifulsoup4\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66azvgl5dt49",
        "outputId": "8177f5a0-23b7-4407-d9b2-628e16f2ec62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for the getting inforamtion of the web page\n",
        "def get_paperinfo(paper_url):\n",
        "    response=requests.get(url,headers=headers)\n",
        "    try:\n",
        "        if response.status_code != 200:\n",
        "            print('Status code:', response.status_code)\n",
        "    \n",
        "    except: print('Failed to fetch web page ')\n",
        "    paper_doc = BeautifulSoup(response.text,'html.parser')\n",
        "    return paper_doc"
      ],
      "metadata": {
        "id": "5QAw4VpwdyaW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for the extracting information of the tags\n",
        "def get_tags(doc):\n",
        "    paper_tag = doc.select('[data-lid]')\n",
        "    cite_tag = doc.select('[title=Cite] + a')\n",
        "    link_tag = doc.find_all('h3',{\"class\" : \"gs_rt\"})\n",
        "    author_tag = doc.find_all(\"div\", {\"class\": \"gs_a\"})\n",
        "    abstract_tag = doc.find_all(\"div\",{\"class\":\"gs_rs\"})\n",
        "    return paper_tag,cite_tag,link_tag,author_tag,abstract_tag"
      ],
      "metadata": {
        "id": "RwOUpCSud2aX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to return the title of the paper\n",
        "def get_papertitle(paper_tag):\n",
        "    paper_names = []\n",
        "    for tag in paper_tag:\n",
        "        paper_names.append(tag.select('h3')[0].get_text())\n",
        "    return paper_names"
      ],
      "metadata": {
        "id": "-g_txNPvd7I8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for the getting author , year and publication information\n",
        "import re\n",
        "def get_author_year_publi_info(authors_tag):\n",
        "    years = []\n",
        "    publication = []\n",
        "    authors = []\n",
        "    for i in range(len(authors_tag)):\n",
        "        authortag_text = (authors_tag[i].text).split()\n",
        "        year = int(re.search(r'\\d+', authors_tag[i].text).group())\n",
        "        years.append(year)\n",
        "        publication.append(authortag_text[-1])\n",
        "        author = authortag_text[0] + ' ' + re.sub(',','', authortag_text[1])\n",
        "        authors.append(author)\n",
        "    return years , publication, authors"
      ],
      "metadata": {
        "id": "GrGpVqv9d9_z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for getting abstract\n",
        "def get_abstract_text(abstract_tag):\n",
        "    abstract = []\n",
        "    for i in range(len(abstract_tag)):\n",
        "        abstracttag_text = (abstract_tag[i].text)\n",
        "        abstract.append(abstracttag_text)\n",
        "    return abstract"
      ],
      "metadata": {
        "id": "YeRjzlIxeB3J"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating final repository\n",
        "import pandas as pd\n",
        "paper_repos_dict = {\n",
        "                    'Paper Title' : [],\n",
        "                    'Year' : [],\n",
        "                    'Author' : [],\n",
        "                    'Publication' : [],\n",
        "                    'Abstract' : []}\n",
        "\n",
        "# adding information in repository\n",
        "def add_in_paper_repo(papername,year,author,publi,abstr):\n",
        "    paper_repos_dict['Paper Title'].extend(papername)\n",
        "    paper_repos_dict['Year'].extend(year)\n",
        "    paper_repos_dict['Author'].extend(author)\n",
        "    paper_repos_dict['Publication'].extend(publi)\n",
        "    paper_repos_dict['Abstract'].extend(abstr)\n",
        "    return pd.DataFrame(paper_repos_dict)"
      ],
      "metadata": {
        "id": "wpHD8NFfeFNz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting final attributes\n",
        "import time\n",
        "for i in range (0,110,10):\n",
        "    url = \"https://scholar.google.com/scholar?q=information+retrieval&hl=en&as_sdt=0%2C5&as_ylo=2012&as_yhi=2022\".format(i)\n",
        "    doc = get_paperinfo(url)\n",
        "    paper_tag,cite_tag,link_tag,author_tag,abstract_tag = get_tags(doc)\n",
        "    papername = get_papertitle(paper_tag)\n",
        "    year , publication , author = get_author_year_publi_info(author_tag)\n",
        "    abstract = get_abstract_text(abstract_tag)\n",
        "    final = add_in_paper_repo(papername,year,author,publication,abstract)\n",
        "    time.sleep(30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0YeMgkaeI6H",
        "outputId": "62d84516-e753-4e15-f459-a1063743ee27"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status code: 429\n",
            "Status code: 429\n",
            "Status code: 429\n",
            "Status code: 429\n",
            "Status code: 429\n",
            "Status code: 429\n",
            "Status code: 429\n",
            "Status code: 429\n",
            "Status code: 429\n",
            "Status code: 429\n",
            "Status code: 429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating final table\n",
        "final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "MuvkPVEweN9H",
        "outputId": "46c9dbb9-5058-438a-d49f-befd5f194b03"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a360b6a0-07fe-44a2-8bed-3c47bef608b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Paper Title</th>\n",
              "      <th>Year</th>\n",
              "      <th>Author</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a360b6a0-07fe-44a2-8bed-3c47bef608b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a360b6a0-07fe-44a2-8bed-3c47bef608b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a360b6a0-07fe-44a2-8bed-3c47bef608b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Paper Title, Year, Author, Publication, Abstract]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating csv file \n",
        "final.to_csv('information_retrieval_reserachpapers.csv', sep=',', index=False,header=True)"
      ],
      "metadata": {
        "id": "iq5yru3veRjT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCsZyBWkWhAO"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wESonytKWhAP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "012387a5-eabd-47b7-8fbe-7c0fca65d102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ac95148bf0d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mnew_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" -filter:retweets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mcsvWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created at\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"User\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Location\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m for tweet in tp.Cursor(api.search_tweets,q=new_search,count=100,\n\u001b[0m\u001b[1;32m     27\u001b[0m                            \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                            since_id=0).items():\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'search_tweets'"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "!pip install tweepy\n",
        "import pandas as pd\n",
        "import tweepy as tp\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re \n",
        "import string\n",
        "\n",
        "consumer_key= '2Em7SxlX9jPMfL4x97r3zMO0x'\n",
        "consumer_secret= 'sVbJzekKuiAgq83Y7gCwNVbSowqQokGVzWexKHl2cXIPceWtSd'\n",
        "access_key= '1439767876962029572-uUMt8oWRyzj9ilE5zk4uYbL93sCMPT'\n",
        "access_secret= 'oydIGymn9bS767FVEMawE9GyGAnMmBJfaY2XXKmHnmliF'\n",
        "\n",
        "auth = tp.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "\n",
        "api = tp.API(auth,wait_on_rate_limit=True)\n",
        " \n",
        "csvFile = open('file-name', 'a')\n",
        "csvWriter = csv.writer(csvFile)\n",
        " \n",
        "search_words = \"#COVID19\"      # enter your words\n",
        "new_search = search_words + \" -filter:retweets\"\n",
        "csvWriter.writerow([\"Created At\",\"Text\",\"User\",\"Location\"])\n",
        "for tweet in tp.Cursor(api.search_tweets,q=new_search,count=100,\n",
        "                           lang=\"en\",\n",
        "                           since_id=0).items():\n",
        "    \n",
        "    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8'),tweet.user.screen_name.encode('utf-8'), tweet.user.location.encode('utf-8')])\n",
        "    data=pd.read_csv('file-name')\n",
        "    data.head(1000)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-02-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}